The basic and most wide spread way of making a machine learning model is a descision tree.

**Pandas**

Pandas is the most prefered python library to read, manipulate any dataset.
Dataframe, The basic format of storage of data using pandas, is like a Excel sheet or a Table in SQL.
> pandas.read_csv("File path") is the method to access the data in form of a Dataframe using pandas.
> df.describe() : it describes the data and provides various values like 
        the min, max, average, etc.for the columns in the dataframes
> df = df.dropna(axis = 0) : is used to drop         null values in the data frame.
A Single data column can be selected from the dataframe using dot-notation. Ex: y = df.column_name
To select Features from the given data , a array of selected features is passed to the df[] dataframe.
Example: Features= [array]
         x = df[Features]
> df.head() : is used to print the few upper rows of the dataset 
> df.column : gives the names of all columns in the dataframe.

----------------------------------------------------------------------------------------------------
**scikit-learn**

While writing in a code, scikit-learn is written as sklearn.

The steps to building and using a model are:

#Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.
#Fit: Capture patterns from provided data. This is the heart of modeling.
#Predict: Just what it sounds like
#Evaluate: Determine how accurate the model's predictions are.

To select the DecisionTreeRegression model using scikit-learn
> model_name = DecisionTreeRegressor(random_state=1)  "Model Defining"
> modelname.fit(Input_data , Prediction_data)  "Model Fitting"

Specifying random_state to get results in each run. We can use any number for this. It is a good practice.

----------------------------------------------------------------------------------------------------

**Model Validation**

The Method of predicting values based on training data and matching it with the value in the target value in the testing data is not a good method.
There are many metrices to evalualte a models quality. One of this is MEAN ABSOLUTE ERROR (MAE).

Prediction error for house price prediction Example
error = actual - predicted

To calculate mean ABSOLUTE error
> from sklearn.metrices import mean_absolute_error
> 
> predicted_value = model_name.predict(Input_data)
> mean_absolute_error(Output , predicted_value)

In-Sample data points
this method have a measure problem when we give a model real-world data.

Example
Imagine that, in the large real estate market, door color is unrelated to home price.
However, in the sample of data you used to build the model, all homes with green doors were very expensive. The model's job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.
Since this pattern was derived from the training data, the model will appear accurate in the training data.
But if this pattern Imagine that, in the large real estate market, door color is unrelated to home price.

However, in the sample of data you used to build the model, all homes with green doors were very expensive. The model's job is to find patterns that predict home prices, so it will see this pattern, and it will always predict high prices for homes with green doors.

Since this pattern was derived from the training data, the model will appear accurate in the training data.

But if this pattern doesn't hold when the model sees new data, the model would be very inaccurate when used in practice.

doesn't hold when the model sees new data, the model would be very inaccurate when used in practice.

the models practical value of accuracy comes from predicting values of new data. the data that was not useds to build the model.
THIS IS CALLED VALIDATION OF DATA.

WE use "train_test_split" function of scikit-learn library

>from sklearn.model_selection import train_test_split
>train_x , train_y , test_x , test_y = train_test_split( x , y , random_state = 0)
>model_name = DecisionTreeRegressor()
>model_name.fit(train_x , train_y)
>predicted_value = model_name.predict(test_x)
>print(mean_absolute_error(test_y , predicted_value))

----------------------------------------------------------------------------------------------------

***UNDERFITTING AND OVERFITTING***

In a decision tree algorithm , the depth of tree is the number of splits the tree went through before it reached to the final prediction.

if we increase the splits of data to a large extent(10 - 20 splits), the tree matches the training data nearly perfectly but, it fails in matching for new freah data.
this phenomenon is called OVERFITTING.

Similarly if a decision tree is too shallow (2-4 splits) ,The tree fails to predict values even for training data.
this is called UNDERFITTING.

In decision tree algorithm , there are some methods to control the depth of tree.
the "max_leaf_nodes" is a argument to control OVERFITTING and UNDERFITTING.
MOre Leaves means less underfitting.

>from sklearn.metrics import mean_absolute_error
>from sklearn.tree import DecisionTreeRegressor

>def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return(mae)

# Overfitting: capturing spurious patterns that won't recur in the future, leading to less accurate predictions, or
# Underfitting: failing to capture relevant patterns, again leading to less accurate predictions.

----------------------------------------------------------------------------------------------------
***RANDOM FOREST***

Decision tree has a limitation, i.e. you can have a deep tree which might result in overfitted model resulting in incampatibility with real world data.
Where as on the other hand you can have a shallow tree which result in poor predictions and fails to capture all distinctions in the dataset.

To overcome this, we use Random Forest Technique. In Random Forest we use multiple trees and make a prediction by averaging the predictions of each component tree.

